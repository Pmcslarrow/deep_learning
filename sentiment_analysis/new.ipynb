{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis (Deep Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import functools\n",
    "import re\n",
    "\n",
    "train_df = pd.read_csv('twitter_training.csv', header=None)\n",
    "val_df = pd.read_csv('twitter_validation.csv', header=None)\n",
    "\n",
    "def rename_columns(df):\n",
    "    return df.rename(columns={\n",
    "        0: 'tweet_id',\n",
    "        1: 'subject',\n",
    "        2:'sentiment_class',\n",
    "        3:'text'\n",
    "    }, inplace=True)\n",
    "\n",
    "def drop_columns(df):\n",
    "    df.drop(\n",
    "        ['subject', \"tweet_id\"],\n",
    "        axis=1,\n",
    "        inplace=True\n",
    "    )\n",
    "\n",
    "def convert_types(df):\n",
    "    df['text'] = df['text'].astype(str).fillna('')\n",
    "\n",
    "rename_columns(train_df)\n",
    "rename_columns(val_df)\n",
    "\n",
    "drop_columns(train_df)\n",
    "drop_columns(val_df)\n",
    "\n",
    "convert_types(train_df)\n",
    "convert_types(val_df)\n",
    "\n",
    "train_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "### Description\n",
    "In this step we will begin to clean the data so that we can make our model more effective in the future. \n",
    "\n",
    "The first thing that we need to take care of are the special characters. Our model can only understand the input as numbers (not text), therefore, our vocabulary shouldn't end up confusing a word like 'good!' with 'good'. They both mean the same thing and having the special character will only hurt our model at the end of the day, especially when we begin to word embed. \n",
    "\n",
    "The second thing that we need to take care of is the punctuation, as everything should be in lower case for the same reason as above so that 'Good' is not different from 'good' for example. \n",
    "\n",
    "### Example Cleaning\n",
    "*Before*:   If you don't know that I'm a huge @ Borderlands fan and Maya is one of my favorite characters,\n",
    "\n",
    "*After*:    if you dont know that im a huge  borderlands fan and maya is one of my favorite characters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    \"\"\"\n",
    "    This function removes special characters and turns everything into lowercase to improve model accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    for i, string in enumerate(df['text']):\n",
    "        if isinstance(string, str):\n",
    "            replacement = re.sub('[^A-Za-z0-9 ]+', '', string)\n",
    "            replacement = replacement.lower()\n",
    "            df.loc[i, 'text'] = replacement\n",
    "preprocessing(train_df)\n",
    "preprocessing(val_df)\n",
    "\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "### Description\n",
    "This is the process of learning the vocabulary of words that exist in our text column, and then turning the string of words into a numeric representation\n",
    "\n",
    "### Methods used\n",
    "*fit_on_texts* is used to update the vocabulary of what exists inside of text\n",
    "\n",
    "*texts_to_sequences* is used to transform each text into a sequence of integers for our model\n",
    "\n",
    "*pad_sequences* is a method that I learned online that is used to keep all sequences the same length to create uniformity. It is likely that the majority of text have different sizes, and this just pads it with zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(df):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "    tokenizer.fit_on_texts(df['text'])\n",
    "    sequences = tokenizer.texts_to_sequences(df['text'])\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "    return tokenizer, padded_sequences\n",
    "\n",
    "train_tokenizer, train_sequences = tokenize(train_df)\n",
    "val_tokenizer, val_sequences = tokenize(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "In our very first row of the training dataset, the word borderlands appears. Below shows the process of how we can actually find where borderlands exists in the tokenized vocabulary. \n",
    "\n",
    "We notice that in the long list of words, that borderlands exists as 129 in numerical form in our padded training sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.head(1), end='\\n\\n------------------------------\\n')\n",
    "print(\"Index in train_sequence:\", train_tokenizer.word_index['borderlands'], end='\\n------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps:\n",
    "\n",
    "I belive it would be best practice to flatten this into a vector and use it as the input into the model, I will take the day to do some more research before coding along aimlessly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_sequences[:2])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
