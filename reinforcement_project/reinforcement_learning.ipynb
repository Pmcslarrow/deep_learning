{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task / Motivation\n",
    "\n",
    "For the last week or so I have been working through MIT's youtube courses for deep learning to get a better understanding of how it worked. I have seen things like RNNs, CNNs, VAEs, and, my personal favorite -- deep reinforcement learning. \n",
    "\n",
    "The goal of this assignment is for educational purposes. I used resources from videos, articles, and more to better understand how these things work. I haven't had a lot of exposure to ML/AI and I thought that this would be an interesting project for me to learn more about what is happening under the hood. \n",
    "\n",
    "While it may be difficult to re-construct exactly, the lessons that I learned deep diving into this subject help me have a high level understanding of how some of these models actually work, and how the math/statistics drive the solutions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup / Environment\n",
    "\n",
    "After doing some research I quickly learned that OpenAI created an API named \"gym\" that is specifically used for reinforcement learning tasks, and it has prebuilt game environments already setup. \n",
    "\n",
    "https://www.gymlibrary.dev/content/basic_usage/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gym in /Users/pmcslarrow/Library/Python/3.9/lib/python/site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /Users/pmcslarrow/Library/Python/3.9/lib/python/site-packages (from gym) (1.25.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/pmcslarrow/Library/Python/3.9/lib/python/site-packages (from gym) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /Users/pmcslarrow/Library/Python/3.9/lib/python/site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /Users/pmcslarrow/Library/Python/3.9/lib/python/site-packages (from gym) (6.8.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Library/Python/3.9/site-packages (from importlib-metadata>=4.8.0->gym) (3.16.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "<TimeLimit<OrderEnforcing<PassiveEnvChecker<FrozenLakeEnv<FrozenLake-v1>>>>>\n"
     ]
    }
   ],
   "source": [
    "#!pip install --upgrade gym\n",
    "import gym\n",
    "\n",
    "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False)\n",
    "print(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action Space\n",
    "0: LEFT   \n",
    "1: DOWN   \n",
    "2: RIGHT   \n",
    "3: UP\n",
    "\n",
    "The reward is given a +1 IFF the player reaches the goal, and 0 otherwise. Notice that in our random experiment below, that it is really rare to get to the end randomly. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0  Score: 0.0\n",
      "Episode: 1  Score: 0.0\n",
      "Episode: 2  Score: 0.0\n",
      "Episode: 3  Score: 1.0\n",
      "Episode: 4  Score: 0.0\n",
      "Episode: 5  Score: 0.0\n",
      "Episode: 6  Score: 0.0\n",
      "Episode: 7  Score: 0.0\n",
      "Episode: 8  Score: 0.0\n",
      "Episode: 9  Score: 0.0\n",
      "Episode: 10  Score: 0.0\n",
      "Episode: 11  Score: 0.0\n",
      "Episode: 12  Score: 0.0\n",
      "Episode: 13  Score: 0.0\n",
      "Episode: 14  Score: 0.0\n",
      "Episode: 15  Score: 0.0\n",
      "Episode: 16  Score: 0.0\n",
      "Episode: 17  Score: 0.0\n",
      "Episode: 18  Score: 0.0\n",
      "Episode: 19  Score: 0.0\n",
      "Episode: 20  Score: 0.0\n",
      "Episode: 21  Score: 0.0\n",
      "Episode: 22  Score: 0.0\n",
      "Episode: 23  Score: 0.0\n",
      "Episode: 24  Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "episodes = 25\n",
    "for episode in range(episodes):\n",
    "    observation, info = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = random.choice([0, 1, 2, 3])\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        score += reward\n",
    "\n",
    "        if terminated or truncated:\n",
    "            done = True\n",
    "    print(f\"Episode: {episode}  Score: {score}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning\n",
    "\n",
    "* This material is for learning purposes and has been better understood thanks to resources such as: https://towardsdatascience.com/q-learning-algorithm-from-explanation-to-implementation-cdbeda2ea187 *\n",
    "\n",
    "The goal of any reinforcement learning task is to maximize the total rewards an agent gets from its environment through a trial and error process. At each step (or state) that the agent is in, it needs to make a decision (an action) of where it can maximize the reward at that step. In our case in the frozen lake example, at each step we are choosing an action step (left, up, right, down) and are rewarded once we can get to the goal. Our model should learn how to interpret what is a good or bad step. \n",
    "\n",
    "In order to make the best action at a step *s*, the agent must find the best probability distribution over which action to take at *s*. \n",
    "\n",
    "Q Learning is an algorithm that learns about how to find the optimal (or maximum) Q-value at this step. To make this possible, Q learning stores all the values in a table that is constantly updated at each step based on the old q value, and the new learned value. \n",
    "\n",
    "*a* is the learning rate.    \n",
    "*r* is the reward for taking action *a* at state *s*.    \n",
    "*\\gamma* is the discount factor    \n",
    "*s'* is the next state after taking the next action    \n",
    "*a'* is the action that maximized the Q value in the next state   \n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "Q(s,a) \\leftarrow (1 - \\alpha) \\cdot Q(s,a) + \\alpha \\cdot \\left( r + \\gamma \\cdot \\max_{a'} Q(s',a') \\right)\n",
    "\\end{equation}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "qtable = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "qtable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration or Exploitation \n",
    "\n",
    "https://gist.github.com/aamrani-dev/fe00597615dae967f5ca1909a6ecf1d2#file-q_learning-py\n",
    "\n",
    "As I learn more about reinforcement learning and Q-learning, it becomes clear that knowing when to explore or exploit is essential. In the early iterations, our reinforcement learning model lacks information about the environment, prompting it to prioritize exploration over exploitation. However, as the model learns with each iteration, the exploration decay constant gradually diminishes the need for extensive exploration! Instead, the model begins to leverage its accumulated knowledge, shifting towards prioritizing exploitation. \n",
    "\n",
    "To choose whether to explore or exploit, we use a uniform distribution between 0 and 1 and if our random number is less than the exploration probability, the agent selects a random action (explores), otherwise, exploits the newfound knowledge using the \"bellman equation\" as explained from the article. (Very similar to how Markov chain Monte Carlo scenarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 15000                 \n",
    "max_iter_episode = 100     \n",
    "min_exploration_proba = 0.01              \n",
    "exploration_proba = 1.0                \n",
    "exploration_decreasing_decay = 0.005\n",
    "gamma = 0.95                         \n",
    "lr = 0.8                         \n",
    "\n",
    "rewards = []\n",
    "for e in range(n_episodes):\n",
    "    current_state = env.reset()[0]\n",
    "    done = False\n",
    "    \n",
    "    total_episode_reward = 0\n",
    "    \n",
    "    for i in range(max_iter_episode): \n",
    "        if np.random.uniform(0,1) < exploration_proba:\n",
    "            action = env.action_space.sample()  # Exploration\n",
    "        else:\n",
    "            action = np.argmax(qtable[current_state,:]) # Exploitation\n",
    "        \n",
    "        next_state, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "\n",
    "        if (terminated or truncated):\n",
    "            done = True\n",
    "        \n",
    "        qtable[current_state, action] = (1-lr) * qtable[current_state, action] + lr * (reward + gamma * max(qtable[next_state, :]))\n",
    "        total_episode_reward = total_episode_reward + reward\n",
    "        if done:\n",
    "            break\n",
    "        current_state = next_state\n",
    "\n",
    "\n",
    "    exploration_proba = max(min_exploration_proba, np.exp(-exploration_decreasing_decay*e))\n",
    "    rewards.append(total_episode_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward per thousand episodes\n",
      "1000 : mean espiode reward:  0.017\n",
      "2000 : mean espiode reward:  0.02\n",
      "3000 : mean espiode reward:  0.016\n",
      "4000 : mean espiode reward:  0.009\n",
      "5000 : mean espiode reward:  0.013\n",
      "6000 : mean espiode reward:  0.013\n",
      "7000 : mean espiode reward:  0.008\n",
      "8000 : mean espiode reward:  0.014\n",
      "9000 : mean espiode reward:  0.015\n",
      "10000 : mean espiode reward:  0.029\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean reward per thousand episodes\")\n",
    "for i in range(10):\n",
    "    print((i+1)*1000,\": mean espiode reward: \", np.mean(rewards[1000*i:1000*(i+1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.01597476e-05 1.21337457e-05 7.04186646e-06 1.53302129e-05]\n",
      " [5.24654053e-07 5.12249747e-07 4.48738417e-07 5.51176790e-07]\n",
      " [1.29826986e-06 1.90563507e-06 2.17000462e-06 5.25098727e-06]\n",
      " [5.08897780e-06 4.69426206e-07 7.91646348e-07 1.27497473e-06]\n",
      " [1.83514974e-07 6.87412232e-06 7.11414411e-04 1.51401163e-07]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [9.78308478e-09 2.67227212e-04 7.81131272e-11 2.82086320e-09]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [7.81271653e-05 8.68620664e-06 2.49666371e-03 6.38562702e-06]\n",
      " [6.95822017e-05 1.06350360e-05 2.68215530e-05 7.02920352e-04]\n",
      " [2.65221281e-05 5.50744598e-05 1.27626441e-05 7.03222006e-05]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [9.31292037e-05 1.82878903e-02 1.28478193e-02 1.33970200e-03]\n",
      " [1.86828242e-02 1.33386564e-02 3.66536333e-01 7.55032010e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
